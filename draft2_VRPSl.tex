\documentclass[journal]{IEEEtran}

\usepackage{ifpdf}
 \ifpdf
   % pdf code
 \else
   % dvi code
 \fi
\usepackage{cite}
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
   %\graphicspath{{../pdf/}{../jpeg/}}
   \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  \usepackage[dvips]{graphicx}
  %\graphicspath{{../eps/}}
  \DeclareGraphicsExtensions{.eps}
\fi
\usepackage[cmex10]{amsmath}
\usepackage{amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{fixltx2e}
\usepackage{url}
\usepackage{color}

\newenvironment{meta}[0]{\color{red} \em}{}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Filtering and Smoothing Algorithms for Nonlinear Variable Rate Models with Applications in Tracking}

\author{Pete~Bunch,~\IEEEmembership{}
        Simon~Godsill,~\IEEEmembership{Member,~IEEE,}% <-this % stops a space
\thanks{P. Bunch and S. Godsill are with the Department
of Engineering, Cambridge University, UK. email: \{pb404,sjg30\}@cam.ac.uk}% <-this % stops a space
\thanks{Manuscript received January 01, 1901; revised January 02, 1901.}}

% The paper headers
\markboth{IEEE Transaction in Signal Processing,~Vol.~1, No.~1, January~1901}%
{Bunch \& Godsill \MakeLowercase{\textit{et al.}}: Filtering and Smoothing Algorithms for Nonlinear Variable Rate Models with Applications in Tracking}

% make the title area
\maketitle

\begin{abstract}
The abstract goes here.
\end{abstract}

\begin{IEEEkeywords}

\end{IEEEkeywords}



\section{Introduction}

\IEEEPARstart{C}{onventional} sequential state-space inference methods use discrete-time hidden Markov models to estimate an unknown quantity evolving over time. These models may be developed by discretisation of a continuous-time model, or by formulation directly in discrete time. In either case, it is common practice to use a grid of time points synchronous with the observations. Such ``fixed rate'' models may give poor representations of processes which exhibit structured behaviour over longer time scales. In such cases, a ``variable rate'' model may prove advantageous - the model is discretised onto a set of set of random times, which characterise the transitions in system behaviour.

Using a variable rate model, it is necessary to estimate both the set of changepoints and the (nonlinearly) evolving state. This is not analytically tractable so numerical approximations must be employed. The particle filter (introduced by \cite{Gordon1993}) and smoother (see \cite{Doucet2000a,Godsill2004}) are such schemes which approximate the posterior state distribution using a set of samples, or ``particles'', drawn sequentially from it.  A thorough introduction to particle filtering and smoothing methods can be found in \cite{Cappe2007,Doucet2009}.

Target tracking algorithms are commonly based upon fixed rate models (see, e.g. \cite{Li2003} for a thorough survey), in which the target kinematics (position, velocity, etc.) are estimated at a set of fixed times at which observations (e.g. radar measurements) are made. In \cite{Godsill2004a,Godsill2007a,Godsill2007}, variable rate models were introduced for tracking, in which the state trajectory is divided up by a set of changepoints between which the motion follows a deterministic path governed by kinematic parameters (accelerations, etc.) which are fixed for that segment. The changepoint times and kinematic parameters may be estimated using a numerical approximation called the variable rate particle filter (VRPF).

In this paper we review the VRPF for piecewise-deterministic nonlinear models and introduce improvements using the resample-move method of \cite{Gilks2001}. We then describe a new variable rate particle smoother (VRPS) algorithm, which uses a novel Markov Chain Monte Carlo (MCMC) formulation to circumvent difficulties of the conventional methods. In section~\ref{}, we adapt the intrinsic-coordinate models of \cite{Godsill2007a,Godsill2007} for use with the new smoothing algorithm. Simulations are presented in section~\ref{}.



\section{Piecewise-Deterministic Variable Rate Models}

We consider a general model from time $0$ to $T$, between which observations, $\{y_1 \dots y_N\}$, are made at times $\{t_1 \dots t_N = T\}$. During this period, an unknown number, $K$, of changepoints occur at times $\{ \tau_1 \dots \tau_K \}$. Discrete sets containing multiple values over time will be written as, e.g. $y_{1:n} = \{y_1 \dots y_n\}$.

Changepoints are treated as random variables, dependent on the previous times. The sequence it assumed to be Markovian throughout this paper.

\begin{IEEEeqnarray}{rCl}
 \tau_{k} & \sim & p(\tau_{k}|\tau_{k-1})   \label{eq:cp_model}
\end{IEEEeqnarray}

In continuous time, the dynamics of the latent state, $x(t)$, may be written as a stochastic differential equation dependent on the sequence of changepoints which have occured so far, $\tau_{1:k}$ (i.e. $\tau_k < t < \tau_{k+1}$).

\begin{IEEEeqnarray}{rCl}
 dx(t) & = & \mathfrak{f}(x(t), v(t), \tau_{1:k})dt \label{eq:cont_time_state_diff_eq}
\end{IEEEeqnarray}

The dynamics depend on the changepoint sequence, $\tau_{1:k}$, and a continuously varying random innovation, $v(t)$. For the variable rate models considered in this paper, this stochastic differential equation will be solved by assuming that the random innovation is constant between changepoints, $v(t) = v_k \forall t : \tau_k < t < \tau_{k+1}$. This means that the state trajectory is a deterministic function dependent on the changepoint times and innovations.

\begin{IEEEeqnarray}{rCll}
 x(t) & = & f(x_{k-1}, v_{k-1}, \tau_{1:k-1}, t) &, \tau_{k-1} < t < \tau_k    \label{eq:disc_time_state_diff_eq}
\end{IEEEeqnarray}

The innovations may be treated as random variables dependent on the previous values. This sequence will also be assumed to be Markovian throughout this paper.

\begin{IEEEeqnarray}{rCl}
 v_{k} & \sim & p(v_{k}|v_{k-1})   \label{eq:innov_model}
\end{IEEEeqnarray}

Once the next changepoint time is known, (\ref{eq:disc_time_state_diff_eq}) may be used to calculate the state at this changepoint, $x_k = x(\tau_k)$. In addition, the state at the observation times will be given by $\hat{x}_n = x(t_n)$, which is needed to evaluate measurement likelihoods.

To keep the following derivations concise, an additional variable is introduced to denote each changepoint time-innovation pair $\theta_k = \{\tau_k, v_k\}$. It will often be assumed unless stated otherwise that the most recent changepoint occured at time $\tau_k$ and that the current innovation value is $v_k$.

It will be useful to denote the set of changepoints and parameters (of unspecified size) which occur within some range of time. This will be written as $\theta_{[s,t]} = \{ \theta_k \forall k : s<\tau_k<t \}$. Such a variable may be thought of as representing the number, $k$, of changepoints within the interval, plus the times and innovations of each changepoint. Note also that such a variable not only conveys where changepoints occur, but also where they do not within the specfied range.



\section{The Variable Rate Particle Filter}

The objective of an inference algorithm will be to estimate the sequence of changepoint times, $\tau_{[0,t_n]}$ and the state trajectory, $x(t) \forall t<t_n$, given some set of the observations, $y_{1:n}$. The state may be calculated deterministically using (\ref{eq:disc_time_state_diff_eq}) once the changepoint time and innovation sequences, $\theta_{[0,t_n]}$, are known (plus the initial state, $x_0$).

In \cite{Godsill2004a,Godsill2007a,Godsill2007}, the variable rate particle filter (VRPF) was introduced for performing this inference sequentially. At time $t_n$, the filter estimates the posterior changepoint distribution.

\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{L}{p(\theta_{[0,t_n]}, x_0|y_{1:n})} \nonumber \\
 & = & \frac{ p(y_n|\theta_{[0,t_n]}, x_0, y_{1:n-1}) p(\theta_{[0,t_n]}, x_0|y_{1:n-1}) }{ p(y_n|y_{1:n-1}) } \nonumber \\
 & = & \frac{ p(y_n|\hat{x}_n) p(\theta_{[t_{n-1},t_n]}|\theta_{[0,t_{n-1}]}) p(\theta_{[0,t_{n-1}]}, x_0|y_{1:n-1}) }{ p(y_n|y_{1:n-1}) }
\end{IEEEeqnarray}

This distribution cannot be calculated analytically, but may be approximated numerically.

A particle filter is an algorithm for approximating a probability distribution using a set of weighted samples (or ``particles'') drawn from that distribution. In this case, each particle will be a set of changepoint times and innovations (plus an initial state).

\begin{equation}
 \hat{p}(\theta_{[0,t_n]}, x_0|y_{1:n}) = \sum_j w_n^{(j)} \delta_{(\theta_{[0,t_n]}^{(j)}, x_0^{(j)})}(\theta_{[0,t_n]}, x_0)
\end{equation}

where $\delta_x(X)$ is a probability mass at the point where $X=x$.

The particle filter works recursively. At the $n$th step, a particle, $\theta_{[0,t_{n-1}]}^{(i)}$, is first resampled from those approximating the filtering distribution at the $(n-1)$th step, using an arbitrary set of proposal weights.

\begin{equation}
 q(\theta_{[0,t_{n-1}]}, x_0) = \sum_j \beta_n^{(j)} \delta_{(\theta_{[0,t_{n-1}]}^{(j)}, x_0^{(j)})}(\theta_{[0,t_{n-1}]}, x_0)
\end{equation}

An extension, $\theta_{[t_{n-1},t_n]}^{(i)}$, is then proposed from an importance distribution, $q(\theta_{[t_{n-1},t_n]}|\theta_{[0,t_{n-1}]}^{(i)}, y_n)$. Finally, the particle is weighted according to the ratio of the target distribution and the proposal.

\begin{IEEEeqnarray}{rCl}
w_n^{(i)} & = & \frac{ p(\theta_{[0,t_n]}^{(i)}, x_0^{(i)}|y_{1:n}) }{ q(\theta_{[0,t_{n-1}]}^{(i)}, x_0^{(i)}|y_{1:{n-1}}) q(\theta_{[t_{n-1},t_{n}]}^{(i)}|\theta_{[0,t_{n-1}]}^{(i)},) } \nonumber \\
    & \propto & \frac{ p(y_{n}|\theta_{[0,t_n]}^{(i)}, x_0^{(i)}, y_{1:n-1}) p(\theta_{[t_{n-1},t_n]}^{(i)}|\theta_{[0,t_{n-1}]}^{(i)}) p(\theta_{[0,t_{n-1}]}^{(i)}, x_0^{(i)}|y_{1:n-1}) }{ q(\theta_{[0,t_{n-1}]}^{(i)}, x_0^{(i)}|y_{1:{n-1}}) q(\theta_{[t_{n-1},t_{n}]}^{(i)}|\theta_{[0,t_{n-1}]}^{(i)}, y_n) } \nonumber \\
    & \approx & \frac{w_{n-1}^{(i)}}{\beta_{n-1}^{(i)}} \times \frac{ p(y_{n}|\hat{x}_n) p(\theta_{[t_{n-1},t_n]}^{(i)}|\theta_{[0,t_{n-1}]}^{(i)}) }{ q(\theta_{[t_{n-1},t_{n}]}^{(i)}|\theta_{[0,t_{n-1}]}^{(i)}, y_n) } \label{eq:VRPF_weights}
\end{IEEEeqnarray}

The normalisation may be enforced by scaling the weights so that they sum to $1$.

The first term, $p(y_{n}|\hat{x}_n)$, is simply an observation likelihood of the deterministically calculated state. The second term, $p(\theta_{[t_{n-1},t_n]}^{(i)}|\theta_{[0,t_{n-1}]}^{(i)})$, is a transition density, the form of which depends on whether or not a new changepoint occurs within the interval $[t_{n-1},t_n]$. Let the occurence of $k$ changepoints already have been estimated in the interval $[0,t_{n-1}]$. It is assumed that the probability of more than one changepoint occuring in the interval $[t_{n-1},t_n]$ is negligible.

\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{p(\theta_{[t_{n-1},t_n]}|\theta_{[0,t_{n-1}]})} \nonumber \\
 & = & \begin{cases} p(\tau_{k+1}|\tau_{k}, \tau_{k+1}>t_{n-1}) p(\tau_{k+2}>t_n|\tau_{k+1}) p(v_{k+1}|v_{k}) & \tau_{k+1}<t_n \\ p(\tau_{k+1}>t_n|\tau_{1:k}, \tau_{k+1}>t_{n-1}) & \tau_{k+1}>t_n \end{cases}
\end{IEEEeqnarray}

This can be calculated from transition models for $\tau_k$ and $v_k$ specified by (\ref{eq:cp_model}) and (\ref{eq:innov_model}). For the most basic ``bootstrap'' \cite{Gordon1993} form of VRPF, this transition density may be used as the importance distribution, leading to the usual simplification in the weight formula.

The choice of proposal weights, $\{\beta_{n-1}^{(i)}\}$, requires particular attention in the design of VRPFs. In some models a changepoint may not have an immediate effect on the observations, especially if a jump occurs in some quantity which is only observed via its integral, e.g. if there is a jump in the acceleration of a moving object, yet only the position is measured, the change will not be apparent until several more observations have been made. In the meantime, particles which contain a changepoint at the correct time may all have been removed by the resampling process. A particle pays a debt in transition probability when a changepoint is proposed, and does not see it repaid in likelihood until later. To avoid this loss of good particles, proposal weights should be chosen which preserve a significant number of low-weight particles. One scheme which has been found to work well is described in \cite{Godsill2007}, in which proposal weights are given by:

\begin{IEEEeqnarray}{rCl}
\beta_{n-1}^{(i)} & \propto & \max ( 1, N_F w_{n-1}^{(i)} )
\end{IEEEeqnarray}

where $N_F$ is the number of filtering particles. The VRPF is summarised below.% in algorithm~\ref{alg:VRPF}.

%\begin{algorithm}
\begin{algorithmic}
\STATE Initialise particles $\{\theta_{[0,0]}^{(i)}\} \gets \emptyset$.
\STATE Sample starting state prior, $\{x_0^{(i)}\} \sim p(x_0)$
\FOR{$n=1 \dots N$}
  \FOR{$i=1 \dots N_F$}
  	\STATE Sample a history $\{\theta_{[0,t_{n-1}]}^{(i)}, x_0^{(i)}\} \sim \sum_j v_n^{(j)} \delta_{(\theta_{[0,t_n]}^{(j)}, x_0^{(j)})}(\theta_{[0,t_n]}, x_0)$.
    \STATE Propose $\theta_{[t_{n-1},t_n]}^{(i)} \sim q(\theta_{[t_{n-1},t_n]}|\theta_{[0,t_{n-1}]}^{(i)}, y_{n})$
    \STATE Calculate $\hat{x}_n$ using (\ref{eq:disc_time_state_diff_eq}).
    \STATE Weight $w_n^{(i)} \propto \frac{w_{n-1}^{(i)}}{\beta_{n-1}^{(i)}} \times \frac{ p(y_{n}|\hat{x}_n) p(\theta_{[t_{n-1},t_n]}^{(i)}|\theta_{[0,t_{n-1}]}^{(i)}) }{ q(\theta_{[t_{n-1},t_{n}]}^{(i)}|\theta_{[0,t_{n-1}]}^{(i)}, y_n) }$
  \ENDFOR
  \STATE Scale weights such that $\sum_i w_n^{(i)}=1$
\ENDFOR
\end{algorithmic}
%\label{alg:RBVRPF}
%\caption{Rao-Blackwellised Variable Rate Particle Filter}
%\end{algorithm}



\subsection{Improved VRPF Using Resample-Move}

The weakness of the ordinary VRPF is that it is very difficult to spot changepoints until after they have actually happened. In the basic filter, changepoints can only be added between the previous and current observation times, $t_{n-1}$ and $t_n$. It is necessary to have a very large number of particles in order to ensure that some propose changepoints in the correct areas.

The filter performance can be greatly improved by the addition of resample-move steps \cite{Gilks2001} to adjust the time of previous changepoints, $\tau_{1:k}$ and innovations, $v_{1:k}$. Here we consider altering only the most recent values.

In a resample-move scheme, Metropolis-Hastings (MH) \cite{Metropolis1953,Gilks1996} steps are included after resampling to regenerate the particle distribution. A proposal is made to change one or more of the previous states, and accepted with a probability that depends on the ratio of target and proposal distribution probabilities. Here, we propose a new value for the most recent changepoint time and the corresponding innovation, $\theta_k^* \sim q(\theta_k|\theta_{1:k-1}, y_{1:n})$, which are used to replace the current values, $\theta_k^{(m)}$, to construct a new changepoint sequence, $\theta_{[0,t_n]}^*$. Thus, the changepoint times and innovations may be changed many frames after they were first proposed, when more informative observations are available. The acceptance probability for such a MH move is the following ratio.

\begin{IEEEeqnarray}{rCl}
 \alpha & = & \frac{ p(\theta_{[0,t_n]}^*, x_0|y_{1:n}) q(\theta_k^{(m)}|\theta_{1:k-1}, y_{1:n}) }{ p(\theta_{[0,t_n]}^{(m)}, x_0|y_{1:n}) q(\theta_k^*|\theta_{1:k-1}, y_{1:n}) } \\
        & = & \frac{ p(y_{s_k:n}|\theta_{[0,t_n]}^*, x_0) p(\theta_{[t_{s_k-1},t_n]}^*|\theta_{[0,t_{s_k-1}]}) q(\theta_k^{(m)}|\theta_{1:k-1}, y_{1:n}) }{ p(y_{s_k:n}|\theta_{[0,t_n]}^{(m)}, x_0) p(\theta_{[t_{s_k-1},t_n]}^{(m)}|\theta_{[0,t_{s_k-1}]}) q(\theta_k^{*}|\theta_{1:k-1}, y_{1:n}) } \\
        & = & \frac{ p(y_{s_k:n}|\hat{x}_{s_k:n}^*) p(\theta_{k}^*|\theta_{1:k-1}) q(\theta_k^{(m)}|\theta_{1:k-1}, y_{1:n}) }{ p(y_{s_k:n}|\hat{x}_{s_k:n}^{(m)}) p(\theta_{k}^{(m)}|\theta_{1:k-1}) q(\theta_k^{*}|\theta_{1:k-1}, y_{1:n}) }
\end{IEEEeqnarray}

Here, $s_k$ is the index of the earliest observation to come after either $\tau_k$ or $\tau_k^*$ (Thus $y_{s_k:n}$ simply means ``all the observations affected by the proposed move''.).

In addition, simpler moves can be conducted in which only $v_k$ is changed, or more complex moves in which multiple past changepoints are altered. The computational complexity for such moves is likely to be high, however, because of the large number of likelihood evaluations to be made. It is also possible to included reversible jump moves \cite{Green1995} to add or remove changepoints entirely.

A proposal distribution for resample-move steps can be formulated using the Unscented Transform \cite{Julier1997} to approximate the ``optimal importance distribution'' \cite{}{\meta Cite someone who explains this term}. Ideally, new values for the random variables should be proposed from the conditional distribution:

\begin{IEEEeqnarray}{rCl}
p(v_k|\theta_{1:k-1}, \tau_k, y_{1:n}) & \propto & p(y_{s_k:n}|\theta_{1:k}) p(v_k)
\end{IEEEeqnarray}

This may be approximated using an Unscented Kalman Filter (UKF) \cite{Julier1997} with no prediction steps. If changepoints occurly sparsely, this can lead to large amounts of computation due to the number of observations in the set $y_{s_k:n}$. In this case, the proposal can be formed using only a subset of the available observations, thus limiting the computational load.



\section{The Variable Rate Particles Smoother}

A filter conducts inference sequentially as new observations are introduced. The purpose of a smoother is to produce a second estimate once all the observations have been made, using future values to improve upon the filter performance. For ordinary fixed rate models, a particle approximation of the joint smoothing distribution may be constructed by resampling values from the filter approximations backwards through time \cite{Godsill2004}.

Formulating an effective smoother for piecewise deterministic variable rate models proves to be a challenging task, because changing an estimate of the innovations or the state in the past will change the state, and thus the observation likelihood, in the future. This obstacle prevents the use of direct backward sampling smoothers, such as that of \cite{Godsill2004}, for which an intractable integral would be required in the weight calculations. In this section, a smoother which uses MCMC to sample the changepoint history is developed, using similar principles to \cite{Bunch2012}.

In the derivation of the VRPF, the filter targetted the joint distribution over changepoint times, $\tau_{[0,t_n]}$, and innovations, $v_{[0,t_n]}$. This was desirable because once these sequences have been estimated, the state can be calculated deterministically for all times up to $t_n$, an essential property for measurement likelihood evaluations. For a smoother, such a scheme is problematic because changing one of the innovations at an early time will alter the state at all subsequent times. Instead, the sequence of innovations, $v_{[0,T]}$, is replaced by the sequence of states occuring at the changepoint times, $x_{[0,T]} = \{ x(\tau_k) \forall k \}$, which may be calculated deterministically from the changepoint time and innovation sequences. Targetting a distribution over this state sequence, if a change is made to an estimate of $x_k$, it is no longer necessary for the rest of the states to change, only to alter the preceeding and following innovations, $v_{k-1}$ and $v_{k}$. For such a scheme to be workable, it must be possible to calculate an innovation from the states and times of the bounding changepoints. In other words, (\ref{eq:disc_time_state_diff_eq}) must be ``invertible''.

\begin{IEEEeqnarray}{rCl}
 v_k & = & f^{-1}(x_{k-1}, x_k, \tau_{k-1}, \tau_{k})  \label{eq:state_invert}
\end{IEEEeqnarray}

Analagously with $\theta$, a new variable is introduced to represent changepoint time-state pairs, $\zeta_k = \{\tau_k, x_k\}$. Thus if $\zeta_{0:K}$ is known (with $\tau_0 = 0$), it is possible to calculate the innovation sequence using (\ref{eq:state_invert}) and thence to calculate the state at any time. Thus, for piecewise-deterministic variable rate models, the posterior distribution targetted by a smoother is $p(\zeta_{[0,T]}|y_{1:N})$. An approximation to this distibution may be derived from the final filtering results, by tranforming from the innovation sequence, $v_{[0,T]}$, to the changepoint state sequence, $x_{[0,T]}$. This approximation will tend to suffer from particle degeneracy due to the resampling procedure, i.e. the number of unique particles decreases for earlier times, giving a poor representation of the distribution \cite{Kitagawa1996}. An improved smoothing algorithm is needed to rejeuvenate this particle diversity.

For an ordinary fixed rate model, particle smoothing may be conducted by sequentially resampling states from the filtering approximations backwards in time. With the nonlinear variable rate models under consideration here, this process is not tractable. Instead, particles are sampled from the joint smoothing distribution using MCMC. The posterior distribution is first expanded using Bayes theorem around observation time $t_n$.

\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{p(\zeta_{[0,T]}|y_{1:N})} \nonumber \\
\qquad & =       & \frac{ p(y_{1:N}|\zeta_{[0,T]}) p(\zeta_{[0,T]}) }{ p(y_{1:N}) } \\
       & \propto & p(\zeta_{[t_n,T]}|\zeta_{[0,t_n]}) p(\zeta_{[0,t_n]}) \nonumber \\
       &         &  \times p(y_{r_k^+ +1:N}|\hat{x}_{r_k^+ +1:N}) p(y_{r_k^-:r_k^+}|\hat{x}_{r_k^-:r_k^+}) p(y_{1:r_k^- -1}|\hat{x}_{1:r_k^- -1})  \IEEEeqnarraynumspace
\end{IEEEeqnarray}

The likelihood term has been split up into three parts, divided at $r_k^-$ and $r_k^+$, the earliest and latest observations to occur between the successive changepoints $\tau_k$ and $\tau_{k+1}$, which lie before and after $t_n$ respectively.

A Markov chain is initialised by sampling a particle from the approximation generated by the final filtering step. Because this is an approximation of the target distribution, no burn-in is requiired. Proceeding backwards in time, MH steps are conducted by proposing a replacement for the changepoint sequence history from the appropriate filtering approximation. At the $n$th step, the future changepoints are considered to be fixed, $\tilde{\zeta}_{[t_n,T]}$. A replacement for the past changepoints is proposed from the filtering particles.

\begin{equation}
 \zeta_{[0,t_n]}^* \sim q(\zeta_{[0,t_n]}|y_{1:n}) = \sum_j \gamma_n^{(j)} \delta_{(\zeta_{[0,t_n]}^{(j)})}(\zeta_{[0,t_n]})
\end{equation}

This proposed move is accepted with the following acceptance probability, where $\zeta_{[0,t_n]}^{(m)}$ is the previous sequence of past changepoints in the Markov chain.

\begin{IEEEeqnarray}{rCl}
 \alpha & = & \frac{ p(\zeta_{[0,t_n]}^*, \tilde{\zeta}_{[t_n,T]}|y_{1:N}) q(\zeta_{[0,t_n]}^{(m)}|y_{1:n}) }{ p(\zeta_{[0,t_n]}^{(m)}, \tilde{\zeta}_{[t_n,T]}|y_{1:N}) q(\zeta_{[0,t_n]}^*|y_{1:n}) } \nonumber \\
        & = & \frac{ \gamma_n^{(m)} }{ \gamma_n^* } \times \frac{ p(\tilde{\zeta}_{[t_n,T]}|\zeta_{[0,t_n]}^*) p(\zeta_{[0,t_n]}^*) }{ p(\tilde{\zeta}_{[t_n,T]}|\zeta_{[0,t_n]}^{(m)}) p(\zeta_{[0,t_n]}^{(m)}) } \nonumber \\
        &   &  \times \frac{ p(y_{r_k^-:r_k^+}|\hat{x}_{r_k^-:r_k^+}^*) p(y_{1:r_k^- -1}|\hat{x}_{1:r_k^- -1}^*) }{ p(y_{r_k^-:r_k^+}|\hat{x}_{r_k^-:r_k^+}^{(m)}) p(y_{1:r_k^- -1}|\hat{x}_{1:r_k^- -1}^{(m)}) } \nonumber \\
        & = & \frac{ \gamma_n^{(m)} }{ \gamma_n^* } \times \frac{ p(\zeta_{[0,t_n]}^*) p(y_{1:r_k^- -1}|\hat{x}_{1:r_k^- -1}^*) }{ p(\zeta_{[0,t_n]}^{(m)}) p(y_{1:r_k^- -1}|\hat{x}_{1:r_k^- -1}^{(m)}) } \nonumber \\
        &   &  \times \frac{ p(\tilde{\zeta}_{k+1}|\zeta_{k}^*) p(y_{r_k^-:r_k^+}|\hat{x}_{r_k^-:r_k^+}^*) }{ p(\tilde{\zeta}_{k+1}|\zeta_{k}^{(m)}) p(y_{r_k^-:r_k^+}|\hat{x}_{r_k^-:r_k^+}^{(m)}) }     \label{eq:MCMC-VRPS_ap}
\end{IEEEeqnarray}

The first fraction is simply a ratio of proposal weights for the current and proposed changepoint histories. The second ratio comprises probabilities which depend solely on the past changepoints, and which are thus the same as values calculated during the filtering stage. These may be stored when first calculated, meaning they can simply be fetched from memory when calculating the acceptance probability.

The third ratio consists of the transition probability between two known changepoints, $\zeta_k$ and $\zeta_{k+1}$, and the likelihood of observations occurring between them. These may be found by calculating the innovation, $v_k$, to get from $\zeta_k$ to $\zeta_{k+1}$. The continuous state trajectory may then be calculated allowing the likelihood terms to be evaluated. The transition density term is given by:

\begin{IEEEeqnarray}{rCl}
 p(\zeta_{k+1}|\zeta_{k}) & = & p(\tau_{k+1}|\tau_{k}) p(v_{k}|v_{k-1})
\end{IEEEeqnarray}

The variable rate particle smoother (VRPS) is summarised below.%in algorithm~\ref{alg:MCMC-VRPS}.

%\begin{algorithm}
 \begin{algorithmic}
  \STATE Run a particle filter to approximate $p(\theta_{[0,t_n]}, x_0|y_{1:n})$ for each $n$.
  \STATE Convert each particle innovation sequence to a changepoint state sequence, to produce approximations of $p(\zeta_{[0,t_n]}|y_{1:n})$.
  \FOR{$i=1 \dots S$}
    \STATE Initialise sampler with $\tilde{\zeta}_{[0,T]}^{(i)} \sim \sum_{j} w_N^{(j)} \delta_{\zeta_{[0,T]}^{(j)}}(\zeta_{[0,T]})$
    \FOR{$n=N \dots 1$}
      \STATE $\zeta_{[0,T]}^{(i)(0)} \gets \tilde{\zeta}_{[0,T]}^{(i)}$.
      \FOR{$m=1 \dots M$}
        \STATE Propose a new past $\zeta_{[0,t_n]}^{(i)*} \sim \sum_j \gamma_n^{(j)} \delta_{(\zeta_{[0,t_n]}^{(j)})}(\zeta_{[0,t_n]})$.
	      \STATE Calculate $\alpha$ using equation~\ref{eq:MCMC-VRPS_ap}
	      \STATE With probability $\alpha$, $\zeta_{[0,t_n]}^{(i)(m)} \gets \zeta_{[0,t_n]}^{(i)*}$. Otherwise $\zeta_{[0,t_n]}^{(i)(m)} \gets \zeta_{[0,t_n]}^{(i)(m-1)}$.
      \ENDFOR
      \STATE $\tilde{\zeta}_{[0,t_n]}^{(i)} \gets \zeta_{[0,t]}^{(i)(M)}$.
    \ENDFOR
  \ENDFOR
 \end{algorithmic}
%\label{alg:MCMC-VRPS}
%\caption{MCMC variable rate particle smoother}
%\end{algorithm}

The number of MH steps, $M$, at each observation time, $n$, allows a trade-off of performance against time. Larger values of $M$ will ensure more unique samples in the smoothing approximation, but will also take longer to execute. In the simulations discussed in this report, $M=1$ was used throughout.



\section{Variable Rate Models for Tracking}

In this section, a tracking model for use with variable rate particle filters and smoothers is presented. In \cite{Godsill2007,Godsill2007a}, a two-dimensional (2D) variable rate model was introduced in which an object experienced piecewise constant random accelerations tangential and normal to the velocity, and a drag force negatively proportional to the velocity. The latent state is a vector consisting of the position in Cartesian coordinates along with the bearing and speed. The resulting differential equations cannot be solved analytically, so the filter relies on numerical integration to calculate the position. This makes it unsuitable for use with the proposed VRPS algorithm, because it is not possible to calculate the accelerations (and thus the transition probabilities) given the state at two successive changepoints.

In addition, the basic intinisic coordinate model is degenerate; there are four state variables (two position, two velocity) but only two parameters governing the intervening motion (the two perpendicular accelerations). Consequently, for two successive changepoints there is likely to be no possible innovation value resulting in the required change. This property makes the model unsuitable for smoothing algorithms.

In this section, the kinematic state at a changepoint will be denoted $\mathbf{x}_{k}$, and the innovation as $\mathbf{v}_{k}$.

The simpler 2D model outlined here is analytic ($\mathbf{x}_{k+1}$ may be calculated without numerical integration given $\mathbf{x}_k$, $\mathbf{v}_k$ and the changepoint times), non-degenerate (there are as many independent random variables for each transition as state variables) and invertible ($\mathbf{v}_k$ may be calculated analytically given $\mathbf{x}_k$ and $\mathbf{x}_{k+1}$.

The state vector consists of cartesian position, $x_t$ and $y_t$, plus bearing, $\psi_t$, and speed, $\dot{s}$.

\begin{equation}
\mathbf{x}_t = [x_t, y_t, \psi_t, \dot{s}_t]^T
\end{equation}

In addition to the tangential and normal accelerations, $a_{T,k}$ and $a_{N,k}$, we introduce two linear drift velocity terms, $d_{X,k}$ and $d_{Y,k}$. Together, these four variables make up the innovation vector:

\begin{equation}
\mathbf{v}_k = [a_{T,k}, a_{N,k}, d_{X,k}, d_{Y,k}]^T
\end{equation}

The target dynamics are described by four differential equations.

\begin{IEEEeqnarray}{rCl}
\ddot{s}_t & = & a_{T,k} \\
\dot{s}_t \dot{\psi}_t & = & a_{N,k} \\
\dot{x}_t & = & \dot{s}_t \cos(\psi_t) + d_{X,k} \\
\dot{y}_t & = & \dot{s}_t \sin(\psi_t) + d_{Y,k}.
\end{IEEEeqnarray}

Solving these yields the following state equations (where $\Delta\tau = \tau_{k+1} - \tau_k$):

\begin{IEEEeqnarray}{rCl}
\dot{s}_{k+1} & = & \dot{s}_k + a_{T,k} \Delta\tau \label{eq:2D_ICmodel_start}\\
\psi_{k+1} & = & \psi_k + \frac{a_{N,k}}{a_{T,k}} \log \left( \frac{\dot{s}_{k+1}}{\dot{s}_k} \right) \\
x_{k+1} & = & x_k + d_{X,k} \Delta\tau + \frac{ \dot{s}_{k+1}^2 }{ 4 a_{T,k}^2 + a_{N,k}^2 } \left[  a_{N,k} \sin(\psi_{k+1}) + 2 a_{T,k} \cos(\psi_{k+1})  \right] \nonumber \\
        &   & - \: \frac{\dot{s}_k^2}{4 a_{T,k}^2 + a_{N,k}^2} \left[  a_{N,k} \sin(\psi_k) + 2 a_{T,k} \cos(\psi_k)  \right] \\
y_{k+1} & = & y_k + d_{Y,k} \Delta\tau + \frac{ \dot{s}_{k+1}^2 }{ 4 a_{T,k}^2 + a_{N,k}^2 } \left[ -a_{N,k} \cos(\psi_{k+1}) + 2 a_{T,k} \sin(\psi_{k+1})  \right] \nonumber \\
        &   & - \: \frac{\dot{s}_k^2}{4 a_{T,k}^2 + a_{N,k}^2} \left[  -a_{N,k} \cos(\psi_k) + 2 a_{T,k} \sin(\psi_k)  \right] . \label{eq:2D_ICmodel_end}
\end{IEEEeqnarray}

Particular care must be taken when $\lim a_{T,k} \rightarrow 0$ or $\lim a_{N,k} \rightarrow 0$ (or both), which can be handled using L'H\^{o}pital's rule or by returning to the equations of motion and re-integrating.

By introducing the drift terms , $d_{X,k}$ and $d_{Y,k}$, the model becomes non-degenerate. These terms represent a linear velocity superimposed onto the nonlinear manoeuvre of the object. Physically, they could represent a genuine effect, e.g. the tide in a ship tracking model or the wind in an aircraft tracking model, or they may simply represent an expected degree of model error.

The system equations may easily be solved to yield $\mathbf{v}_k$ from $\mathbf{x}_k$ and $\mathbf{x}_{k+1}$, which is essential for use with the VRPS algorithm.

For the simulations, the innovations are assumed to be Gaussian distributed.



\subsection{Observation Model}

A radar-type, polar observation model was used. Assuming that the coordinate system is centered on the sensor, the observations consist of bearing and range, with the optional addition of bearing and range rate. For this paper, the drift velocities, $d_{X,k}$ and $d_{Y,k}$ are assumed to represent genuine physical effects, and are thus included in the velocity measurements.

\begin{IEEEeqnarray}{rCl}
 \mathbf{y}_n & = & \begin{cases}[b_n, r_n]^T & \text{position measurement only} \\ [b_n, r_n, \dot{b}_n, \dot{r}_n]^T & \text{position and velocity measurements}\end{cases} \\
 b_n          & = & \tan^{-1}(y_n/x_n) + \epsilon_{1,n} \\
 r_n          & = & \sqrt{y_n^2 + x_n^2} + \epsilon_{2,n} \\
 \dot{b}_n    & = & \frac{(\dot{s}_n \cos(\psi_n) +d_{X,k})y_n + (\dot{s}_n \sin(\psi_n)+d_{Y,k})x_n}{y_n^2 + x_n^2} + \epsilon_{3,n} \\
 \dot{r}_n    & = & \frac{(\dot{s}_n \cos(\psi_n)+d_{X,k})x_n + (\dot{s}_n \sin(\psi_n)+d_{Y,k})y_n}{\sqrt{y_n^2 + x_n^2}} + \epsilon_{4,n}
\end{IEEEeqnarray}

The observation noise terms, $\epsilon_{i,n}$ are assumed to be independent and Gaussian distributed.



\section{Simulations}

The VRPF and VRPS algorithms were tested on simulated data. An example trajectory using the 2D intrinsic-coordinate model is shown in figure~\ref{fig:2D_trajectory}. Figures~\ref{fig:2D_states} and~\ref{fig:2D_observations} show individual states and observations.

\begin{figure}[hbt]
\centering \includegraphics[width=0.5\columnwidth]{D:/pb404/VariableRateAlgorithms/Results_for_paper/VRPFS/2D_WithLinVel_RadarPosOnly_RS1_trajectory.eps}
\caption{Example trajectory simulated from 2D model.}
\label{fig:2D_trajectory}
\end{figure}

\begin{figure}[hbt]
\centering \includegraphics[width=0.5\columnwidth]{D:/pb404/VariableRateAlgorithms/Results_for_paper/VRPFS/2D_WithLinVel_RadarPosOnly_RS1_states.eps}
\caption{Example states simulated from 2D model.}
\label{fig:2D_state}
\end{figure}

\begin{figure}[hbt]
\centering \includegraphics[width=0.5\columnwidth]{D:/pb404/VariableRateAlgorithms/Results_for_paper/VRPFS/2D_WithLinVel_RadarPosOnly_RS1_observations.eps}
\caption{Example observations simulated from 2D model.}
\label{fig:2D_observations}
\end{figure}

The VRPF was run using 50 particles and a RM step with a UKF proposal whenever a particle is replicated through resampling. The VRPS was used to simulated 50 smoothing trajectories.

Figure~\ref{fig:2D_particles} shows the particle position estimates from the final VRPF frame and the VRPS.

\begin{figure}[hbt]
\subfloat[VRPF]{\includegraphics[width=0.45\columnwidth]{D:/pb404/VariableRateAlgorithms/Results_for_paper/VRPFS/2D_WithLinVel_RadarPosOnly_RS1_filtparts.eps}}
\subfloat[VRPS]{\includegraphics[width=0.45\columnwidth]{D:/pb404/VariableRateAlgorithms/Results_for_paper/VRPFS/2D_WithLinVel_RadarPosOnly_RS1_smoothparts.eps}}
\caption{VRPF and VRPS particle position estimates for an example 2D trajectory}
\label{fig:2D_particles}
\end{figure}

Figure~\ref{fig:2D_jumpkd} shows a kernel density estimate of the changepoint times for the VRPF and VRPS.

\begin{figure}[hbt]
\centering \includegraphics[width=0.5\columnwidth]{D:/pb404/VariableRateAlgorithms/Results_for_paper/VRPFS/2D_WithLinVel_RadarPosOnly_RS1_jumpkd.eps}
\caption{VRPF (top) and VRPS (bottom) kernel density estimates of jump times.}
\label{fig:2D_jumpkd}
\end{figure}

While the VRPF generally overestimates the number of jumps, the VRPS is remarkably accurate. In this example, the VRPF estimate 15.1 changepoints (average over all particles), while the VRPS estimates 10.9. The true value is 11.

State estimates were formed by averaging the state values from each particle at the observation times. Root mean square error values can then be calculated for the algorithms. By this metric, the VRPS does not improve upon the performance of the VRPF. RMSEs over time for the example case are shown in figure~\ref{fig:2D_rmse}

\begin{figure}[hbt]
\centering \includegraphics[width=0.5\columnwidth]{D:/pb404/VariableRateAlgorithms/Results_for_paper/VRPFS/2D_WithLinVel_RadarPosOnly_RS1_rmse.eps}
\caption{RMSE over time in position and velocity estimates for the filter, KS and VRPS.}
\label{fig:2D_rmse}
\end{figure}

The benefit of the VRPS is in the improved particle diversity of the state estimates. This is illustrated in figure~\ref{fig:2D_uniquepts}, which shows the number of unique state trajectories up to each time, i.e. the number of unique estimates of $\theta_{[0,t]}$ for each $t$.

\begin{figure}[hbt]
\centering \includegraphics[width=0.5\columnwidth]{D:/pb404/VariableRateAlgorithms/Results_for_paper/VRPFS/2D_WithLinVel_RadarPosOnly_RS1_uniquepts.eps}
\caption{Number of unique particles over time.}
\label{fig:2D_uniquepts}
\end{figure}



\begin{meta}
\begin{itemize}
	\item Presumably there should be some comparison of batch results. What?
	\item Label the graphs better
	\item Add VRPF without RM
\end{itemize}
\end{meta}



% Template bits

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.



\section{Conclusion}
Improved filtering and smoothing algorithms have been presented for inference with variable rate models. The addition of resample-move steps to the basic variable rate particle filter allows the algorithm to run with a fraction of the number of particles. A smoother for estimation of the joint posterior distribution is not tractable using conventional backwards resampling methods. The algorithm derived instead uses MCMC to resample particles from the filter approximations. The resulting smoother does not necessarily improve the RMSE of state estimates, but the number of unique particles in the approximation is increased significantly.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
\section{}
Appendix one text goes here.

\section{}
Appendix two text goes here.


% use section* for acknowledgement
\section*{Acknowledgment}


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliographystyle{IEEEtran}
\bibliography{D:/pb404/Dropbox/PhD/OTbib}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiographynophoto}{Pete Bunch}
Biography text here.
\end{IEEEbiographynophoto}

% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{Simon Godsill}
Biography text here.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


